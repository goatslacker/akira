rx = {
  @IDENTIFIER /^([a-zA-Z_$][0-9a-zA-Z_\-]*[\?]?)/
  @KEY /^(@[a-zA-Z_$][0-9a-zA-Z_\-]*[\?]?)/
  @NUM /^(-?([0-9]+[\.e])?[0-9]+,?)+/
  @STR /^'(.*?)'/
  @ARGS /^&([0-9]+)/
  @LAMBDA /^->/
  @OPERATION /^(<\+|<=|>=|>|<|(!|=)==?|\|\||\&\&|!!|\+\+|\+:)+/
  @TERMINATOR /^\n/
  @REGEX /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  @COMMENTS /^(--.*)/
  @WHITESPACE /^ /
}

identifiers = {
  @KEYWORDS ['if' 'then' 'else' 'import' 'export' 'try' 'catch' 'none'
    'fn' 'raise' 'prototype' 'recur' 'macro' 'cond']
  @TYPE ['Function' 'Number' 'String' 'Array' 'Object' 'Null']
  @COMPARE ['==' '!=' '>' '>=' '<' '<=' 'is' 'isnt']
  @LEFT_OPERATORS ['++' '!!']
  @RIGHT_OPERATORS ['+:']
  @NOT ['not']
  @LOGIC ['&&' '||']
  @BOOL ['true' 'false']
}

ignore = { @COMMENTS @WHITESPACE }

get-type = fn [a] rx | (find: (fn [x] x.test: a))

extract = fn [key chunk] rx !! key | (.exec: chunk)

parse-alphanumeric = fn [t chunk (n = 1)] extract: t chunk | (!! n)

return-identifier = fn
  ['KEYWORDS' item] [(item.toUpperCase!) item]
  [type item] [type item]

parse-identifier = fn [t chunk] {
  item = extract: t chunk
  token = identifiers | (find: (fn [x] x.indexOf: item.2 | (>= 0)))
  return-identifier: token || t item.2
}

parse-regular-expressions = fn [t chunk] {
  item = extract: t chunk
  regexp = RegExp: item.3 item.4
  ['REGEXP' regexp]
}

get-token = fn
  ['IDENTIFIER' chunk] parse-identifier &!
  ['KEY' chunk] parse-identifier &!
  ['NUM' chunk] parse-alphanumeric: t chunk 0
  ['STR' chunk] parse-alphanumeric: t chunk
  ['OPERATION' chunk] parse-identifier &!
  ['LAMBDA' _] 'FN'
  ['REGEX' chunk] parse-regular-expressions &!
  ['COMMENTS' chunk] parse-alphanumeric: t chunk
  ['ARGS' chunk] [t (extract: t chunk | (!! 1))]
  ['WHITESPACE' _] ' '
  ['TERMINATOR' _] ' '
  [t chunk] [chunk.1 chunk.1]

process-chunk = fn [code n] {
  type = get-type: code
  token = get-token: type code
  if Array.isArray: token
    then {
      token.push: n.line
      token
    }
    else [type token n.line]
}

get-length = fn
  ['STR' i] i + 2
  ['ARGS' i] i + 1
  [else] i

incr-line = fn
  ['TERMINATOR' line] line + 1
  [else] line

get-next-index = fn [token] {
  @index (token.2.toString! | (.length) | get-length: token.1)
  @token token
}

return-tokens = fn
  [code n] return-tokens: code n []
  ['' _ tokens] tokens
  [else] {
    {@index @token} = process-chunk: code n | get-next-index
    tokens.push: token
    recur
      (code.substr: index code.length),
      { @line (incr-line: token.1 n.line) },
      tokens
  }

filter-whitespace = fn [token] not (ignore !! token.1)

export tokenize = fn [code] return-tokens: code { @index 0 @line 0 } |
  (.filter: filter-whitespace)
