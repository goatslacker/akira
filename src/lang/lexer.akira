rx = {
  @IDENTIFIER /^([a-zA-Z_$][0-9a-zA-Z_\-]*[\?]?)/
  @KEY /^(@[a-zA-Z_$][0-9a-zA-Z_\-]*[\?]?)/
  @NUM /^(-?([0-9]+[\.e])?[0-9]+,?)+/
  @STR /^'(.*?)'/
  @ARGS /^&([0-9]+)/
  @LAMBDA /^->/
  @OPERATION /^(<-|<=|>=|>|<|(!|=)==?|\|\||\&\&|!!|\+\+|\+:|::)/
  @TERMINATOR /^\n/
  @REGEX /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  @COMMENTS /^(--.*)/
  @WHITESPACE /^ /
}

identifiers = {
  @KEYWORDS ['if' 'then' 'else' 'import' 'export' 'try' 'catch' 'none' 'do'
    'let' 'fn' 'raise' 'prototype' 'recur' 'macro' 'cond' 'match' 'maybe'
    'Maybe']
  @COMPARE ['==' '!=' '>' '>=' '<' '<=' 'is' 'isnt']
  @LEFT_OPERATORS ['++' '!!']
  @RIGHT_OPERATORS ['+:']
  @DO_OPERATOR ['<-']
  @TYPE_OPERATOR ['::']
  @NOT ['not']
  @LOGIC ['&&' '||']
  @BOOL ['true' 'false']
}

ignore = { @COMMENTS @WHITESPACE }

get-type = fn [a] {
  rx | find: -> (&0.test: a)
}

extract = fn [key chunk] {
  rx !! key | (.exec: chunk)
}

parse-alphanumeric = fn [t chunk (n = 1)] {
  extract: t chunk | (!! n)
}

return-token = fn [type] { -> ([type &0]) }

return-identifier = match {
  ['KEYWORDS' item] [(item.toUpperCase!) item]
  [type item] [type item]
}

parse-identifier = fn [t chunk] {
  item = extract: t chunk
  token = identifiers | find: -> (&0.indexOf: item.2 | (>= 0))
  return-identifier: (token || t) item.2
}

convert-regexp = fn [item] { RegExp: item.3 item.4 }
return-token-regexp = return-token: 'REGEXP'
parse-regular-expressions = -> (extract &! | convert-regexp | return-token-regexp)

get-token = fn [type chunk] {
  cond
    type is 'IDENTIFIER' ||
    type is 'KEY' ||
    type is 'OPERATION' ? parse-identifier &!
    type is 'NUM' ? parse-alphanumeric: type chunk 0
    type is 'STR' ||
    type is 'COMMENTS' ? parse-alphanumeric &!
    type is 'LAMBDA' ? 'FN'
    type is 'REGEX' ? parse-regular-expressions &!
    type is 'ARGS' ? [type (extract: type chunk | (!! 1))]
    type is 'WHITESPACE' ||
    type is 'TERMINATOR' ? ' '
    else ? [chunk.1 chunk.1]
}

process-chunk = fn [code n] {
  type = get-type: code
  token = get-token: type code
  if token is Vector
    then token ++ [n.line]
    else [type token n.line]
}

get-length = match {
  ['STR' i] i + 2
  ['ARGS' i] i + 1
  [else] i
}

incr-line = match {
  ['TERMINATOR' line] line + 1
  [else] line
}

get-next-index = fn [token] {
  { @index (token.2.toString! | (.length) | get-length: token.1)
  @token token }
}

return-tokens = match {
  [code n] return-tokens: code n []
  ['' _ tokens] tokens
  [else] {
    {@index @token} = process-chunk: code n | get-next-index
    tokens.push: token
    recur
      (code.substr: index code.length),
      { @line (incr-line: token.1 n.line) },
      tokens
  }
}

filter-whitespace = fn [token] {
  not (ignore !! token.1)
}

tokenize :: String -> Vector
tokenize = fn [code] {
  return-tokens: code { @index 0 @line 0 } |
    (.filter: filter-whitespace)
}

module.exports = tokenize
