rx = {
  IDENTIFIER = /^([a-zA-Z_$][0-9a-zA-Z_\-$]*)/
  NUMBER = /^((\d+\.)?\d+)+/
  STRING = /^'(.*?)'/
  LAMBDA = /^->/
  OPERATION = /^(<=|>=|>|<|==|!=|\|\||\&\&|≤|≥)+/
  TERMINATOR = /^\n/
  REGEX = /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
#  javascriptStart = /^<%/
#  javascriptEnd = /^%>/
  COMMENTS = /^#/
  WHITESPACE = /^ /
}

identifiers = {
  KEYWORDS = ['arguments', 'construct', 'if', 'then', 'else', 'import', 'export']
  COMPARE = ['==', '!=', '>', '>=', '<', '<=', 'is', 'not']
  LOGIC = ['and', 'or', '&&', '||']
  BOOLEAN = ['true', 'false']
  UTILS = [
    'arrEq'
    'assert'
    'at'
    'call'
    'consts'
    'div'
    'eq'
    'filter'
    'flip'
    'foldl'
    'foldr'
    'get'
    'head'
    'init'
    'last'
    'length'
    'map'
    'mod'
    'neq'
    'partial'
    'pop'
    'print'
    'prod'
    'range'
    'sub'
    'sum'
    'square'
    'tail'
  ]
}


filter-tokens-fn(a) -> partial: filter, (= x -> call: (get: rx, x), 'test', a)

get-type(a) -> (
  filter-tokens = filter-tokens-fn: a
  rx | Object.keys | filter-tokens | pop
)

extract(key, chunk) -> call: (get: rx, key), 'exec', chunk

parse-alphanumeric(t, chunk) -> (
  exec = extract: t, chunk
  exec.2
)

filter-identifiers-fn(iden) -> partial: filter, (= x -> neq: 0 - 1, (call: (get: identifiers, x), 'indexOf', iden))

parse-identifier(t, chunk) -> (
  item = extract: t, chunk
  filter-identifiers = filter-identifiers-fn: item.2
  token = identifiers | Object.keys | filter-identifiers | pop
  if token
    then [token, item.2]
    else [t, item.2]
)

parse-regular-expressions(t, chunk) -> (
  item = extract: t, chunk
  regexp = construct RegExp: item.3, item.4
  ['REGEXP', regexp]
)

get-token(chunk, t) =
  'IDENTIFIER' -> parse-identifier: t, chunk
  'NUMBER' -> parse-alphanumeric: t, chunk
  'STRING' -> parse-alphanumeric: t, chunk
  'OPERATION' -> parse-identifier: t, chunk
  'LAMBDA' -> '->'
  'REGEX' -> parse-regular-expressions: t, chunk
  'WHITESPACE' -> ' '
  'TERMINATOR' -> ' '
  otherwise -> [chunk.1, chunk.1]

process-chunk(code, line) -> (
  type = get-type: code
  token = get-token: code, type
  if Array.isArray: token
    then (
      token.push: line
      token
    )
    else [type, token, line]
)

get-length(i, len, type) =
  'STRING' -> i + len + 2
  otherwise -> i + len

incr-line(line, type) =
  'TERMINATOR' -> line + 1
  otherwise -> line

return-tokens(code, i, line) -> (
  tokens = []
  len = length: code
  chunk = code.substr: i, len

  if chunk
    then (
      token = process-chunk: chunk, line
      tokens.push: token
      tokens.concat: (return-tokens: code, (get-length: i, (length: (token.2.toString: ())), token.1), (incr-line: line, token.1))
    )
    else tokens
)

tokenize(code) -> (
  ignore = ['COMMENTS', 'WHITESPACE']
  tokens = return-tokens: (code.trim: ()), 0, 1
  filter: (= token -> eq: 0 - 1, (ignore.indexOf: token.1)), tokens
)

print: (tokenize: '2 + 1\n3 + 4')
