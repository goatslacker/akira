rx = ({
  IDENTIFIER = /^([a-zA-Z_$][0-9a-zA-Z_\-]*)/
  NUM = /^((\d+\.)?\d+)+/
  STR = /^'(.*?)'/
  LAMBDA = /^->/
  OPERATION = /^(<\+|<=|>=|>|<|=>|(!|=)==?|\|\||\&\&|!!|\+\+|\+:)+/
  TERMINATOR = /^\n/
  REGEX = /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  COMMENTS = /^(#.*)/
  WHITESPACE = /^ /
})

identifiers = ({
  KEYWORDS = ['if', 'then', 'else', 'import', 'export', 'try', 'catch', 'none',
    'raise', 'object', 'clones', 'clone', 'prototype']
  TYPE = ['Function', 'Number', 'String', 'Array', 'Object', 'Null']
  COMPARE = ['==', '!=', '>', '>=', '<', '<=', 'is', 'isnt']
  LEFT_OPERATORS = ['++', '!!', '<+']
  RIGHT_OPERATORS = ['=>', '+:']
  NOT = ['not']
  LOGIC = ['and', 'or', '&&', '||']
  BOOL = ['true', 'false']
})

ignore = ['COMMENTS', 'WHITESPACE']

get-type = \a -> rx |
  Object.keys |
  filter: (\x -> rx !! x | (.test: a)) |
  last

extract = \key, chunk -> rx !! key | (.exec: chunk)

parse-alphanumeric = \t, chunk -> extract: t, chunk | (!! 1)

return-identifier = \type, item ->
  'KEYWORDS' ? [(item.toUpperCase!), item]
  else ? [type, item]

parse-identifier = \t, chunk -> {
  item = extract: t, chunk
  token = identifiers |
    Object.keys |
    filter: (\x -> identifiers !! x | (.indexOf: item.2) | (>= 0)) |
    last
  return-identifier: token || t, item.2
}

parse-regular-expressions = \t, chunk -> {
  item = extract: t, chunk
  regexp = clone RegExp: item.3, item.4
  ['REGEXP', regexp]
}

get-token = \t, chunk, n ->
  'IDENTIFIER' ? parse-identifier: t, chunk
  'NUM' ? parse-alphanumeric: t, chunk
  'STR' ? parse-alphanumeric: t, chunk
  'OPERATION' ? parse-identifier: t, chunk
  'LAMBDA' ? '->'
  'REGEX' ? parse-regular-expressions: t, chunk
  'COMMENTS' ? parse-alphanumeric: t, chunk
  'WHITESPACE' ? ' '
  'TERMINATOR' ? ' '
  else ? [chunk.1, chunk.1]

process-chunk = \code, n -> {
  type = get-type: code
  token = get-token: type, code, n
  if Array.isArray: token
    then {
      token.push: n.line
      token
    }
    else [type, token, n.line]
}

get-length = \token, i ->
  'STR' ? i + 2
  else ? i

incr-line = \type, line ->
  'TERMINATOR' ? line + 1
  else ? line

return-tokens = \code, n -> {
  chunk = code.substr: n.index, code.length

  if chunk
    then {
      token = process-chunk: chunk, n
      tokens = token +: []
      n2 = ({
        index = (token.2.toString! |
          (.length) |
          (+ n.index) |
          get-length: token.1)
        line = (incr-line: token.1, n.line)
      })
      tokens ++ (return-tokens: code, n2)
    }
    else []
}

filter-whitespace = \token -> ignore.indexOf: token.1 | (< 0)

export tokenize = \code -> return-tokens: code, ({ index = 0, line = 0 }) |
  filter: filter-whitespace
