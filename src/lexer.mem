rx = {
  IDENTIFIER =: /^([a-zA-Z_$][0-9a-zA-Z_\-]*)/
  NUMBER =: /^((\d+\.)?\d+)+/
  STRING =: /^'(.*?)'/
  LAMBDA =: /^->/
  OPERATION =: /^(<\+|<=|>=|>|<|=>|(!|=)==?|\|\||\&\&|!!|\+\+|\+:)+/
  TERMINATOR =: /^\n/
  REGEX =: /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  COMMENTS =: /^(#.*)/
  WHITESPACE =: /^ /
}

identifiers = {
  KEYWORDS =: ['arguments', 'construct', 'if', 'then', 'else', 'import', 'export', 'try', 'catch', 'none']
  COMPARE =: ['===', '!==', '==', '!=', '>', '>=', '<', '<=', 'is', 'isnt']
  LEFT_OPERATORS =: ['++', '!!', '<+']
  RIGHT_OPERATORS =: ['=>', '+:']
  NOT =: ['not']
  LOGIC =: ['and', 'or', '&&', '||']
  BOOL =: ['true', 'false']
}

ignore = ['COMMENTS', 'WHITESPACE']

get-type = \a -> rx | Object.keys | filter: (\x -> rx !! x | @.test: a), @ | last

extract = \key, chunk -> rx !! key | @.exec: chunk

parse-alphanumeric = \t, chunk -> extract: t, chunk | @.2

filter-identifiers-fn = \iden -> partial: filter, (\x -> identifiers !! x | @.indexOf: iden | @ >= 0)

return-identifier = \item, type
  'KEYWORDS' -> [(item.toUpperCase:!), item]
  else -> [type, item]


parse-identifier = \t, chunk {
  item = extract: t, chunk
  filter-identifiers = filter-identifiers-fn: item.2
  token = identifiers | Object.keys | filter-identifiers | last

  return-identifier: item.2, token || t
}

parse-regular-expressions = \t, chunk {
  item = extract: t, chunk
  regexp = construct RegExp: item.3, item.4
  ['REGEXP', regexp]
}

get-token = \chunk, n, t
  'IDENTIFIER' -> parse-identifier: t, chunk
  'NUMBER' -> parse-alphanumeric: t, chunk
  'STRING' -> parse-alphanumeric: t, chunk
  'OPERATION' -> parse-identifier: t, chunk
  'LAMBDA' -> '->'
  'REGEX' -> parse-regular-expressions: t, chunk
  'COMMENTS' -> parse-alphanumeric: t, chunk
  'WHITESPACE' -> ' '
  'TERMINATOR' -> ' '
  else -> [chunk.1, chunk.1]

process-chunk = \code, n {
  type = get-type: code
  token = get-token: code, n, type
  if Array.isArray: token
    then {
      token.push: n.line
      token
    }
    else [type, token, n.line]
}

get-length = \i, token
  'STRING' -> i + 2
  else -> i

incr-line = \line, type
  'TERMINATOR' -> line + 1
  else -> line

return-tokens = \code, n {
  tokens = []
  len = length: code
  chunk = code.substr: n.index, len

  if chunk
    then {
      token = process-chunk: chunk, n
      tokens.push: token
      n2 = {
        index =: (token.2.toString:!;.length | n.index + @ | get-length: @, token.1)
        line =: (incr-line: n.line, token.1)
      }
      tokens ++ (return-tokens: code, n2)
    }
    else tokens
}

filter-whitespace = \token -> ignore.indexOf: token.1 | 0 > @

export tokenize = \code -> return-tokens: code, { index =: 0, line =: 0 } | filter: filter-whitespace, @
