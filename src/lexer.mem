rx = ({
  IDENTIFIER = /^([a-zA-Z_$][0-9a-zA-Z_\-]*)/
  NUMBER = /^((\d+\.)?\d+)+/
  STRING = /^'(.*?)'/
  LAMBDA = /^->/
  OPERATION = /^(<\+|<=|>=|>|<|=>|(!|=)==?|\|\||\&\&|!!|\+\+|\+:)+/
  TERMINATOR = /^\n/
  REGEX = /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  COMMENTS = /^(#.*)/
  WHITESPACE = /^ /
})

identifiers = ({
  KEYWORDS = ['arguments', 'construct', 'if', 'then', 'else', 'import', 'export', 'try', 'catch', 'none']
  COMPARE = ['===', '!==', '==', '!=', '>', '>=', '<', '<=', 'is', 'isnt']
  LEFT_OPERATORS = ['++', '!!', '<+']
  RIGHT_OPERATORS = ['=>', '+:']
  NOT = ['not']
  LOGIC = ['and', 'or', '&&', '||']
  BOOL = ['true', 'false']
})

ignore = ['COMMENTS', 'WHITESPACE']

get-type = \a -> rx | Object.keys | filter: (\x -> rx !! x | @.test: a), @ | last

extract = \key, chunk -> rx !! key | @.exec: chunk

parse-alphanumeric = \t, chunk -> extract: t, chunk | @.2

return-identifier = \item, type
  'KEYWORDS' -> [(item.toUpperCase:!), item]
  else -> [type, item]

parse-identifier = \t, chunk {
  item = extract: t, chunk
  token = identifiers | Object.keys | filter: (\x -> identifiers !! x | @.indexOf: item.2 | @ >= 0), @ | last

  return-identifier: item.2, token || t
}

parse-regular-expressions = \t, chunk {
  item = extract: t, chunk
  regexp = construct RegExp: item.3, item.4
  ['REGEXP', regexp]
}

get-token = \chunk, n, t
  'IDENTIFIER' -> parse-identifier: t, chunk
  'NUMBER' -> parse-alphanumeric: t, chunk
  'STRING' -> parse-alphanumeric: t, chunk
  'OPERATION' -> parse-identifier: t, chunk
  'LAMBDA' -> '->'
  'REGEX' -> parse-regular-expressions: t, chunk
  'COMMENTS' -> parse-alphanumeric: t, chunk
  'WHITESPACE' -> ' '
  'TERMINATOR' -> ' '
  else -> [chunk.1, chunk.1]

process-chunk = \code, n {
  type = get-type: code
  token = get-token: code, n, type
  if Array.isArray: token
    then {
      token.push: n.line
      token
    }
    else [type, token, n.line]
}

get-length = \i, token
  'STRING' -> i + 2
  else -> i

incr-line = \line, type
  'TERMINATOR' -> line + 1
  else -> line

return-tokens = \code, n {
  chunk = code.substr: n.index, code.length

  if chunk
    then {
      token = process-chunk: chunk, n
      tokens = token +: []
      n2 = ({
        index = (token.2.toString:! | @.length | n.index + @ | get-length: @, token.1)
        line = (incr-line: n.line, token.1)
      })
      tokens ++ (return-tokens: code, n2)
    }
    else []
}

filter-whitespace = \token -> ignore.indexOf: token.1 | 0 > @

export tokenize = \code -> return-tokens: code, ({ index = 0, line = 0 }) | filter: filter-whitespace, @
