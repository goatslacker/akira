rx = {
  IDENTIFIER = /^([a-zA-Z_$][0-9a-zA-Z_\-$]*)/
  NUMBER = /^((\d+\.)?\d+)+/
  STRING = /^'(.*?)'/
  LAMBDA = /^->/
  PIPE = /^=>/
  OPERATION = /^(<=|>=|>|<|==|!=|\|\||\&\&|≤|≥)+/
  TERMINATOR = /^\n/
  REGEX = /^(\/((?![\s=])[^[\/\n\\]*(?:(?:\\[\s\S]|\[[^\]\n\\]*(?:\\[\s\S][^\]\n\\]*)*])[^[\/\n\\]*)*)\/)([imgy]{0,4})(?!\w)/
  COMMENTS = /^(#.*)/
  WHITESPACE = /^ /
}

identifiers = {
  KEYWORDS = ['arguments', 'construct', 'if', 'then', 'else', 'import', 'export', 'try', 'catch']
  COMPARE = ['==', '!=', '>', '>=', '<', '<=', 'is', 'not']
  LOGIC = ['and', 'or', '&&', '||']
  BOOL = ['true', 'false']
  UTILS = [
    'arrEq'
    'assert'
    'at'
    'call'
    'consts'
    'div'
    'eq'
    'filter'
    'flip'
    'foldl'
    'foldr'
    'get'
    'head'
    'init'
    'last'
    'length'
    'map'
    'mod'
    'neq'
    'partial'
    'pop'
    'print'
    'prod'
    'raise'
    'range'
    'sub'
    'sum'
    'square'
    'tail'
  ]
}


filter-tokens-fn(a) -> partial: filter, (= x -> get: rx, x | call: @, 'test', a)

get-type(a) -> (
  filter-tokens = filter-tokens-fn: a
  rx | Object.keys | filter-tokens | pop
)

extract(key, chunk) -> get: rx, key | call: @, 'exec', chunk

parse-alphanumeric(t, chunk) -> (
  exec = extract: t, chunk
  exec.2
)

filter-identifiers-fn(iden) -> partial: filter, (= x -> get: identifiers, x | call: @, 'indexOf', iden | neq: 0 - 1, @)

return-identifier(item, type) =
  'KEYWORDS' -> [(item.toUpperCase: ()), item]
  otherwise -> [type, item]


parse-identifier(t, chunk) -> (
  item = extract: t, chunk
  filter-identifiers = filter-identifiers-fn: item.2
  token = identifiers | Object.keys | filter-identifiers | pop

  return-identifier: item.2, token || t
)

parse-regular-expressions(t, chunk) -> (
  item = extract: t, chunk
  regexp = construct RegExp: item.3, item.4
  ['REGEXP', regexp]
)

get-token(chunk, t) =
  'IDENTIFIER' -> parse-identifier: t, chunk
  'NUMBER' -> parse-alphanumeric: t, chunk
  'STRING' -> parse-alphanumeric: t, chunk
  'OPERATION' -> parse-identifier: t, chunk
  'LAMBDA' -> '->'
  'PIPE' -> '=>'
  'REGEX' -> parse-regular-expressions: t, chunk
  'COMMENTS' -> parse-alphanumeric: t, chunk
  'WHITESPACE' -> ' '
  'TERMINATOR' -> ' '
  otherwise -> [chunk.1, chunk.1]

process-chunk(code, line) -> (
  type = get-type: code
  token = get-token: code, type
  if Array.isArray: token
    then (
      token.push: line
      token
    )
    else [type, token, line]
)

get-length(i, len, type) =
  'STRING' -> i + len + 2
  otherwise -> i + len

incr-line(line, type) =
  'TERMINATOR' -> line + 1
  otherwise -> line

return-tokens(code, i, line) -> (
  tokens = []
  len = length: code
  chunk = code.substr: i, len

  if chunk
    then (
      token = process-chunk: chunk, line
      tokens.push: token
      tokens.concat: (return-tokens: code, (get-length: i, (length: (token.2.toString: ())), token.1), (incr-line: line, token.1))
    )
    else tokens
)

tokenize(code) -> (
  ignore = ['COMMENTS', 'WHITESPACE']
  return-tokens: code, 0, 1 | filter: (= token -> ignore.indexOf: token.1 | eq: 0 - 1, @), @
)

export tokenize
