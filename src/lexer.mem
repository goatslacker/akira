TOKEN = regexp: '^([a-zA-Z]+)'
NUMBER = regexp: '^(\d*\.?\d+)+'
#string = regexp: '^\'(.*?)\''
lambda = regexp: '^->'
operation = regexp: '^(<=|>=|>|<|==|!=|\|\||\&\&|≤|≥)+'
newline = regexp: '^\n'
comments = regexp: '^#'
whitespace = regexp: '^ '

KEYWORDS = ['arguments', 'if', 'then', 'else', 'import', 'export']
COMPARE = ['==', '!=', '>', '>=', '<', '<=', 'is', 'not', '≤', '≥']
LOGIC = ['and', 'or', '&&', '||']
BOOLEAN = ['true', 'false']

processToken(code, lineNo) -> (
  item = at: (call: TOKEN, 'exec', code), 2
  index = call: KEYWORDS, 'indexOf', item
  isKeyword = index >= 0

  if isKeyword
    then (
      key = call: item, 'toUpperCase'
      [key, item, lineNo]
    )
    else ['IDENTIFIER', item, lineNo]
)

types = ['TOKEN', 'NUMBER']

getType(code, cb) -> (
  type = filter: (\type -> type), types
  at: type, 1
)

tokenize(code, lineNo) -> (
  tokens = [null]

  len = length: code

  if len == 0
    then tokens
    else (
      match work
        'TOKEN' -> processToken

      type = getType: code

      fn = work: type
      item = fn: code, lineNo
      print: item

      if item
        then (
          push: tokens, item
          i = length: (at: item, 2)
        )
        else (
          i = 1
        )

      chunk = call: code, 'substr', i, len
      tokens = call: tokens, 'concat', (tokenize: chunk, lineNo)
      tokens
    )
)

generateTokens(code) -> tokenize: (call: code, 'trim'), 0


Lexer = { tokenize = generateTokens }

export Lexer
